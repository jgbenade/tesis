\chapter{The design of a distributed computing project}
% put these two lines after every \chapter{} command
\vspace{-2em}
\minitoc

Chapter 4 introduces the concept of distributed computing by  giving its history and a summary of some well-known projects, together with their results and the frameworks they were built on. The different distribution frameworks that are available today will then be surveyed.  The working of Berkeley Open Infrastructure for Network Computing, in particular, will be examined in detail, as well as all the requirements for a BOINC project.  The chapter will conclude in specifying the design choices neccessary to create a  BOINC project for the enumeration of main classes of \lat s, for example how to break the search tree up into workunits, how to assign workunits to a user based on system specifications etc.

\section{A brief history of public-resource computing}
In John Brunner's 1975 science-fiction novel, \emph{The Shockwave Rider} \cite{brunner}, the main character creates a "tapeworm" program which was able to propagate itself thro	ugh a network of computers, replicating when needed and consuming resources wherever available. This type of program, capable of harnessing resources on any number of machines, piqued the interest of a group of researchers at Xerox's Palo Alto Research Center (which was famously also the home of the first graphical user interface) who set about creating a number of "worms" programs for controlling multi-machine performance measurements of their pioneering first Ethernet network \cite{worms}. 
Distributed computing systems were mainly confined to networks within organisations or academic departments for the following two decades, until  widespread public adoption of personal computers and the internet made it possible to involve the public in distributed computing, or what will be called \emph{volunteer computing}.

Anderson \emph{et al.} \cite{boincwiki} defines volunteer computing as a form of computing where both organizations and members of the public can donate unused computing resources to \emph{projects}, which are usually applications of a scientific or academic nature. Volunteer computing is built largely on trust and mutual goodwill as it is necessary that the volunteers trust the projects not to perform unapproved calculations with their resources and to guard their personal account details carefully. Similarly, although IP and email addresses may be linked to individual volunteers, volunteers remain largely anonymous and there are no disciplinary steps available against a volunteer who wilfully corrupts computations. The main advantages of volunteer computing are, firstly, that it provides access to at least part of the approximately 10 billion devices linked to the internet \cite{cisco, anderson2013} and, secondly, that it raises public awareness of scientific endeavours and provides a mechanism through which a project of large popular appeal but little funding may flourish \cite{boincwiki}.
Volunteer computing is somewhat similar to two other forms of distributed computing namely \emph{grid computing} and \emph{peer-to-peer} networks. Grid computing is generally considered a paradigm through with computing resources are shared within and between organizations in a mutually beneficial way, for example through desktop grids made up of  all the desktop computers within an organization. This has many things in common with volunteer computing but differs in that the "volunteers" are usually more reliable because of a lack of anonymity and the accompanying existence of disciplinary measures \cite{boincwiki, fostergrid}. The principles of peer-to-peer computing, on the other hand, is  perhaps best seen in file-sharing services such as Napster or torrents - data transfer takes place between computers without any form of coordination by central servers \cite{peer, peer2}. Volunteer computing relies on central servers hosting the project and it is not possible for two clients to communicate directly without coordination by the central server. 
 

There are, however, also many challenges involved with publicly distributed computing, for example, the platforms on which your application must be able to execute accurately are a lot more heterogeneous  and the network is likely to be much more unreliable than within a corporation or laboratory.   Despite these factors, scientists were increasingly interested in unlocking to the massive computing power locked away in the idle computer cycles of the public and two early volunteer computing projects, the Great Internet Mersenne Prime Search (GIMPS), which searches for very large Mersenne primes \cite{gimps}, and \texttt{distributed.net}, which facilitates brute-force decryption, were created in 1996 and 1997, respectively. 

GIMPS continues to this day and, according to the project's homepage, currently runs on $737\,759$ CPUs around the world, with an average annual throughput of 129.2 TFLOP/s. Mersenne primes of the form $2^n-1$ for integer  values of $n$ account for most of the very large prime numbers; the first Mersenne prime discovered with GIMPS, $2^{1\,398\,269}-1$ was found in November 1996 \cite{hayes} and the  forty-eight Mersenne prime, and the largest known prime number, $2^{57\,885\,161} -1$ was recently found on 25 January 2013 \cite{gimps}. \texttt{distributed.net} also remains active today and concerns itself with finding optimal Golomb rulers (combinatorial curiosities with application to the placement of radio antennas in astronomy) and deciphering encoded messages \cite{distnet}, a trend which, according to Hayes \cite{hayes}, started when RSA Data Security company issued a number of challenges in 1997, hoping to test whether their encryption systems are breakable and demonstrate the inefficiencies of rival schemes. Notable early successes were the breaking of RSA-129, which involved the factoring of a 129-bit number similar to those commonly used in the RSA encryption scheme and  took 600 volunteers eight months to do, and the decryption of a message encrypted with the Data Encryption Standard (DES), which was developed under US government sponsorship in the 1970s \cite{distnet}. 
Another early project, which has grown into one of the most influential volunteer computing projects, is   SETI@Home (Search for Extraterrestrial Intelligence), launched  in 1999 by a group of scientists from the University of California to examine radio waves picked up by a telescope operated by Cornell University and the National Science Foundation in Arecibo, Puerto Rico \cite{anderson:seti2002}. The project was very well received by the public and attracted millions of volunteers, by 2004 the average sustained processing power of the SETI@Home project was more than 70 TeraFLOPS (1 TeraFLOPS of computing power is equivalent to $10^{12}$ floating point operations per second), more than double the 35 TeraFLOPS of the most powerful supercomputer at that stage, the NEC Earth Simulator \cite{anderson2004boinc}.
 


According to Anderson \emph{et al.} \cite{anderson:seti2002}, the encouraging success of these early projects lead to a wider push for frameworks which could be used for public-resource or large-scale distributed computing. In 1999 the Global Grid Forum was formed as an umbrella corporation for a number of distributed projects collectively called \emph{The Grid} \cite{fostergrid} for resource sharing amongst research organizations, while many private organizations, including Platform Computing and United Devices were developing corporate systems for distributed storage and computation. The biggest milestone in the history of public-resource was, perhaps, the launch of \emph{Berkeley Open Infrastructure for Network Computing} (or BOINC) in 2004 \cite{anderson2004boinc}, spearheaded by David Anderson, who was the Chief Science Officer at the time at United Devices and also co-founder of the SETI@Home volunteer computing project. 

BOINC acts as middle-ware between the scientists and the public, handling the required network connections automatically and thereby drastically decreasing the difficulty of setting up a new project, providing scientists with a comprehensive \emph{application programming Interface} (API) with which to interact with volunteers and letting volunteers use the same client to connect to multiple volunteer computing projects. The first project to make use of BOINC was, unsurprisingly, SETI@Home and a large number of scientific projects followed with applications ranging from testing Einstein's theory of general relativity (Einstein@Home) \cite{eah} and finding new arrangements into which proteins may fold themselves (Folding@Home) \cite{fah} to the distributed rendering of animated films (BURP) \cite{burp}.

More than 80\% of currently active public-resource computing projects make use of BOINC  and a number of large organizations are making use of volunteer computing to further science in a myriad of different ways. Examples include IBM's \emph{World Community Grid} (WCG) initiative which serves the community by computing vaccines for malaria and modelling the earth's fresh-water supply \cite{wcg}, the LHC@Home project based at CERN which processes vast quantities of data obtained from experiments in the Large Hadron Collider \cite{lhcah}, as well as the Climatepridiction.net project which is based at Oxford University in the UK and models runs a number of different models estimating the effects of climate change \cite{cpdn}.

BOINC remains under active development by a group of software developers and project administrators. A BOINC client for Google's open-source mobile operating system Android has recently been released, thereby enabling the approximately 470 million Android smartphones in the market \cite{mobithinking}, many of which have two, four or even eight processors,      to take part in volunteer computing \cite{android}.

\section{Berkeley Open Infrastructure for Network Computing}
Boinc goals: reduce barriers of entry to public resource computing, share resources among autonomous projects, support diverse applications, reward participants

Applications \cite{anderson:pc} x6

Science project, files, database, workunits, results are sent to clients, results returned, compared, credit,  security issues
\subsection{Basic workflow}

files, database, workunits, results are sent to clients, results returned, compared, credit,  security issues, trickle messages
flowchart
\subsection{The components of a distributed computing project}
boinc api
validator, scheduler, assimilator, workgenerator, application (gridenabling, checkpointing etc)

\section{Designing a BOINC project for the enumeration of MOLS}

Specifics about rgidenabling, splitting up the search tree,  checkpointing, validating, workgenerations, handling inconsistent subtree sizes.1


\section{challenges}
demographics: 92\% male in \cite{anderson:pc}



\section{Chapter summary}